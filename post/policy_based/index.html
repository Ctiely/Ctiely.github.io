<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" (三)强化学习 &middot;  Ctiely的个人博客" />
  	<meta property="og:site_name" content="Ctiely的个人博客" />
  	<meta property="og:url" content="https://ctiely.github.io/post/policy_based/" />


    <meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2019-04-24T19:58:43&#43;08:00" />
    <style type="text/css">
        p {text-indent: 2em;text-align: justify;margin: 0 auto;}
        a {text-decoration:none;}
        img {vertical-align:middle; text-align: center;}
    </style>
    
    

  <title>
      Reinforcement Learning &middot; 基于策略的强化学习
  </title>

    <meta name="description" content="这是ctiely的个人博客" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://ctiely.github.io/images/user.png">
	  <link rel="apple-touch-icon" href="https://ctiely.github.io/images/apple-touch-icon.png" />
    
    <link rel="stylesheet" type="text/css" href="https://ctiely.github.io/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="https://ctiely.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Ctiely的个人博客" />
      
      
    
    <meta name="generator" content="Hugo 0.47" />

    <link rel="canonical" href="https://ctiely.github.io/post/policy_based/" />

    
<div id="particles-js"></div>
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="https://ctiely.github.io/js/particles.js"></script>
    <script src="../../js/click.js"></script>
</head>
<body class="nav-closed">

  


 <div class="site-wrapper">



<header class="main-header " style="background-image: url(https://ctiely.github.io/images/user.jpg)">

    <nav class="main-nav overlay clearfix">
        <a class="blog-logo" href="https://ctiely.github.io/"><img src="https://ctiely.github.io/images/user.png" alt="Blog Logo" /></a>
    </nav>
<div class="vertical">
        <div class="main-header-content inner">
            <h1 class="page-title">
              <a class="btn-bootstrap-2 title-scroll" href="#content">强化学习系列</a>
          </h1>
        </div>
</div>
    <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
</header>

  <main id="content" class="content" role="main">


  <article class="post post">

    <header class="post-header">
        <h1 class="post-title" id="policy_based">(三)基于策略的强化学习</h1>
        <section class="post-meta">
        
	<time class="post-date" datetime="2019-04-24">
            2019-04-24
          </time>
        
         
        </section>
    </header>

    <p class="post-content">
      <p>
        相比基于价值的方法，基于策略的方法不需要显式的估计每个{状态，动作}对的Q值，通过估计策略函数中的参数，利用训练好的策略模型进行
        决策。由于采用随机策略函数可以为agent提供探索环境的能力，不需要采用epsilon-greedy策略就可以对环境进行探索，因此基于策略的强化学习
        算法通常使用随机策略函数进行决策。
      </p>
      <h4>一.Policy Gradient算法(PG)</h4>
      <p>
          Policy-Gradient算法的目标是最大化agent与环境交互过程中的整个轨迹(trajectory)上的期望累计回报，即<br><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\max_{\theta} E_{\tau \sim \pi_\theta}[R(\tau)]" width="160"/><br>
      </p>
      <h6>1.策略梯度的推导</h6>
      <p><br>
          现在我们来计算目标函数对模型参数&theta;的导数：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="../images/pg_gradient.png" width="400"><br>
          回顾<a href="../rl_introduction/index.html#traj" target="_blank" style="text-decoration: underline">第一节</a>介绍的基本概念，可以计算得到轨迹的概率：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?P(\tau|\theta) = \rho_0 (s_0) \prod_{t = 0}^T P(s_{t+1}|s_t, a_t) \pi_\theta (a_t|s_t)" width="450"/><br>
          对概率公式取对数可得：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t = 0}^T (\log P(s_{t+1}|s_t, a_t) + \log \pi_\theta (a_t|s_t))" width="650"/><br>
          对数概率公式对参数求导可得：<br>
          <img src="../images/gradient.png" width="700"><br>
          根据上述步骤，可以获得Policy Gradient算法中的目标函数对模型参数&theta;的梯度为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\nabla_\theta J(\pi_\theta) = E_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau) \right]" width="450"/><br>
          当梯度公式中R(&tau;)的确定后，就可以利用梯度提升等算法更新模型参数：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}" width="320"/><br>
      </p>
      <h6>2.回报的不同形式</h6>
      <p><br>
          (1)累计回报或者累计折扣回报<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的Policy Gradient算法中的累计回报采用第一节介绍的累计回报或者累计折扣回报，即：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?R(\tau) = \sum_{t=0}^{T} r_t  \ \ \ or \ \ \ R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t" width="360"/><br>

          (2)reward-to-go Policy Gradient<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在使用第一节介绍的累计回报或者累计折扣回报训练模型时，由于参数的梯度与整条轨迹的总体回报有关，模型并不知道整个交互过程
          中哪些行动带来了更多的回报，这样就会导致模型无法很快的找到真正有价值的动作，从而使得模型的收敛速度缓慢。使用reward-to-go替代累计
          回报，可以一定程度上加快模型的收敛。<br>
          reward-to-go定义为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\hat{R}_t \triangleq \sum_{t^\prime = t}^{T} R(s_{t^\prime}, a_{t^\prime}, s_{t^\prime + 1})" width="240"/><br>
          因此，策略梯度为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\nabla_\theta J(\pi_\theta) = E_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \sum_{t^\prime = t}^{T} R(s_{t^\prime}, a_{t^\prime}, s_{t^\prime + 1}) \right]" width="600"/><br>

          (3)对reward-to-go进行中心化<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于任何一个只依赖状态s<sub>t</sub>的函数b(s<sub>t</sub>)，都满足：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?E_{a_t \sim \pi_\theta} \left[\nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t) \right] = 0" width="360"/><br>
          因此，策略梯度可以改写为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\nabla_\theta J(\pi_\theta) = E_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum_{t^\prime = t}^{T} R(s_{t^\prime}, a_{t^\prime}, s_{t^\prime + 1}) - b(s_t) \right) \right]" width="600"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大量经验表明，当b(s<sub>t</sub>)=V<sup>&pi;</sup>(s<sub>t</sub>)时，能够降低策略梯度估计的方差，提高模型收敛速度，增强模型稳定性。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而在实际使用中，每个状态的价值V<sup>&pi;</sup>(s<sub>t</sub>)是未知的，需要对价值进行估计。使用另一个深度学习模型V<sub>&phi;</sub>(s<sub>t</sub>)对价值函数进行估计，模型的目标函数为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\phi_k = \arg \min_{\phi} E_{s_t, \hat{R}_t \sim \pi_k} \left[ \left( V_\phi(s_t) - \hat{R}_t \right)^2 \right]" width="400"/><br>

          (4)Vanilla Policy Gradient<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VPG算法用优势函数代替策略梯度中的累计回报，此时策略梯度改写为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\nabla_\theta J(\pi_\theta) = E_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t, a_t) \right]" width="500"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4.1)优势函数的估计<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;定义TD残差为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\delta_t^V = r_t + \gamma V(s_{t + 1}) - V(s_t)" width="300"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;记<br>
          <img src="http://latex.codecogs.com/svg.latex?\hat{A}_t^{(k)} \triangleq \sum_{l=0}^{k-1} \gamma^l \delta_{t+l}^V = -V(s_t) + r_t + \gamma r_{t+1} + ... + \gamma^{k-1} r_{t+k-1} + \gamma^k V(s_{t+k})" width="700"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则广义优势估计(generalized advantage estimator)定义为：<br>
          &nbsp;
          <img src="../images/gae.png" width="700"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比回报函数的其他形式，优势函数更能体现不同动作的好坏差异，因此在基于策略的强化学习算法中经常使用。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4.2)VPG算法伪代码<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="../images/vpg.png" width="650"/><br>
      </p>
      <h4>二.Trust Region Policy Optimization算法(TRPO)</h4>
      <p>
          上述介绍的Policy-Gradient算法关注点在于模型的参数，在使用梯度提升等算法时，需要控制模型更新的步长，以使模型逐渐提升，避免出现
          模型参数剧烈变化的情况；然而在强化学习问题中，在参数层面上对模型进行微小的调整后，对应到策略函数层面，也可能会使新旧策略函数之间
          产生巨大的差异，从而使得agent在环境中的表现相差巨大。为了提高策略函数估计的稳定性，Schulman提出了TRPO算法。
      </p>
      <h6>1.重要性抽样</h6>
      <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="../images/importance_samp.png" width="350"/><br>
          根据重要性抽样，Policy Gradient算法的目标函数等价于<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\mathnormal{J}( \theta) = \mathnormal{E}_{\tau \sim \pi_{\theta_\mathnormal{old}}} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\theta_\mathnormal{old}}(a|s)} \mathnormal{R}(\tau) \right]" width="300"/><br>
      </p>
      <h6>2.K-L散度</h6>
      <p>
          K-L散度用来度量两个概率分布的接近程度，描述了用近似分布q去代替原始分布p时产生的信息损失，公式如下<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?D_{KL}(p || q) = E(\log p(x) - \log q(x))" width="320"/><br>
          对于离散分布，K-L散度记为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?D_{KL}(p || q) = \sum_{i = 1}^{N} \left[ p(x_i) \log \frac{p(x_i)}{q(x_i)} \right]" width="320"/><br>
      </p>
      <h6>3.TRPO算法</h6>
      <p>
          TRPO算法与前述介绍的PG算法的不同之处在于：TRPO算法不是通过控制模型参数间接控制新旧策略函数之间的差异，而是通过控制新旧策略函数
          之间的K-L散度直接控制新旧策略函数之间的差异。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRPO算法的目标函数为<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\mathcal{L}(\theta_k, \theta) = E_{s, a \sim \pi_{\theta_k}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}} (s, a) \right]" width="420"/><br>
          目标函数也被称为替代优势函数(surrogate advantage)，度量了用新策略函数替代旧策略函数所能获得的优势。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRPO算法的约束条件为<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\bar{D}_{KL}(\theta || \theta_k) = E_{s \sim \pi_{\theta_k}} \left[ D_{KL}(\pi_\theta (\cdot | s) || \pi_{\theta_k}(\cdot | s)) \right] \leq \delta" width="500"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，TRPO算法参数求解通过下式进行<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="../images/trpo.png" width="360"/><br>
      </p>
      <h6>4.TRPO算法参数求解</h6>
      <p>
          上述TPRO算法求解困难，利用泰勒展开对目标函数和约束条件进行近似<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\mathcal{L}(\theta_k, \theta) \approx g^T (\theta - \theta_k)" width="240"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\bar{D}_{KL}(\theta || \theta_k) \approx \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k)" width="360"/><br>
          通过拉格朗日对偶问题可求得近似问题的解析解<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\theta_{\mathnormal{k}+1} = \theta_{\mathnormal{k}} + \sqrt{\frac{2 \delta}{\mathnormal{g}^{\mathnormal{T}} \mathnormal{H}^{-1} \mathnormal{g}}} \mathnormal{H}^{-1} \mathnormal{g}" width="360"/><br>
          但是需要注意的是，近似问题的解析解并不一定能够严格满足原始TRPO问题的约束条件(即K-L散度)，因此，还需要对模型参数进行线性回溯使得
          新的策略函数&pi;<sub>&theta;<sub>k+1</sub></sub>能够满足以下两个条件：1.满足K-L散度的约束条件，2.使目标函数增大。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\theta_{\mathnormal{k}+1} = \theta_{\mathnormal{k}} + \alpha^{\mathnormal{j}} \sqrt{\frac{2 \delta}{\mathnormal{g}^{\mathnormal{T}} \mathnormal{H}^{-1} \mathnormal{g}}} \mathnormal{H}^{-1} \mathnormal{g}" width="360"/><br>
          其中j是使得新策略函数&pi;<sub>&theta;<sub>k+1</sub></sub>满足上述两个条件的最小的非负整数。
      </p>
      <h6>5.TRPO算法伪代码</h6>
      <p>
          <img src="../images/trpo_algo.png" width="660"/><br>
      </p>
      <h4>三.Proximal Policy Optimization算法(PPO)</h4>
      <p>
          PPO算法是对TRPO算法的近似，TRPO算法利用一阶和二阶信息，使用牛顿法对模型参数进行更新；而PPO算法只使用一阶信息，使用梯度提升法对
          模型参数进行更新。PPO算法与TRPO算法都需要解决的问题，是控制新旧策略函数之间的变化不能太大，为此，PPO算法使用截断函数来代替TRPO算法中的硬约束：K-L散度。
      </p>
      <h6>1.截断替代优势函数</h6>
      <p>
          截断替代优势函数(clipped surrogate advantage)使用某个超参数&epsilon;将替代优势函数(surrogate advantage)截断，用来防止新
          策略函数相对于旧策略函数变化太大，可以使得策略函数在较小的变化中逐渐提升。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截断替代优势函数定义为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?clip \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}} (s, a)" width="360"/><br>
      </p>
      <h6>2.PPO算法目标函数</h6>
      <p>
          PPO算法的目标函数为<br>
          <img src="http://latex.codecogs.com/svg.latex?L(s, a, \theta_k, \theta) = \min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}} (s, a), \ clip \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}} (s, a) \right)" width="720"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;令<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="../images/ppo_clip.png" width="300"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则原始目标函数可以改写为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?L(s, a, \theta_k, \theta) = \min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}} (s, a), \ g(\epsilon, A^{\pi_{\theta_k}} (s, a)) \right)" width="600"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>2.1</b> 当Advantage大于等于0时，目标函数变为<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?L(s, a, \theta_k, \theta) =  \min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, \ (1 + \epsilon) \right)A^{\pi_{\theta_k}} (s, a)" width="500"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;当优势值A是非负数时，由于算法的目标是最大化目标函数L，因此应当增大新策略&pi;<sub>&theta;</sub>(a|s)，由于式中是对两个值中
          的最小者进行最大化，所以当满足&pi;<sub>&theta;</sub>(a|s)>(1+&epsilon;)&pi;<sub>&theta;<sub>k</sub></sub>(a|s)时，目标函数
          变成常数(1+&epsilon;)A<sup>&pi;<sub>&theta;<sub>k</sub></sub></sup>(s,a)，目标函数不会随着新策略函数继续增大而增大，因此，新策略
          函数不再增大，这样就达到了控制策略函数变化太大的目的。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>2.2</b> 当Advantage小于0时，目标函数变为<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?L(s, a, \theta_k, \theta) =  \max \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, \ (1 - \epsilon) \right)A^{\pi_{\theta_k}} (s, a)" width="500"/><br>
          当优势值是负数时，目标函数会随着新策略函数的减小而增大，当满足&pi;<sub>&theta;</sub>(a|s)<(1-&epsilon;)&pi;<sub>&theta;<sub>k</sub></sub>(a|s)时，目标
          函数变成常数(1-&epsilon;)A<sup>&pi;<sub>&theta;<sub>k</sub></sub></sup>(s,a)，同上面的讨论一致，此时新策略函数不再减小，从而起到控制策略函数变化太大的目的。
      </p>
      <h6>3.PPO算法伪代码</h6>
      <p>
          <img src="../images/ppo_algo.png" width="660"/><br>
      </p>
      <h6>4.PPO算法的优势</h6>
      <p>
          PPO算法除了具有基于策略的强化学习算法的共同优势：
      <ol>
          <li>模型具有较好的收敛性，收敛速度较快</li>
          <li>在连续动作空间或者高维动作空间中仍然适用</li>
          <li>通过训练随机策略可以对环境进行探索</li>
      </ol>
          以外，还拥有
      <ol>
          <li>算法容易实现</li>
          <li>样本利用效率较高<br>
              (原因：PPO算法会利用旧策略生成的数据对策略模型进行多次更新，从而提高了样本的利用效率)</li>
      </ol>
      正是因为这些优势，使得PPO算法成为以OpenAI为首的基于策略的强化学习算法中的最强算法。
      </p>
      </p>



  <footer class="post-footer">


    
    <figure class="author-image">

        <a class="img" href="https://ctiely.github.io/" style="background-image: url(https://ctiely.github.io/images/user.png)"><span class="hidden">ctiely</span></a>
    </figure>
    

    <section class="author">

        <p>@Author:<a href="https://github.com/Ctiely" style="text-decoration: underline" target="_blank"><b>ctiely</b></a></p>
        <p>@School:<b>Renmin University of China </b></p>
        <p>@Reference:<a href="https://spinningup.openai.com/en/latest/" style="text-decoration: underline" target="_blank"><b>Spinning Up</b></a></p>

    </section>
  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Ctiely的个人博客</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="https://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/syui/hugo-theme-air">hugo-theme-air</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/index.js"></script>

</body>
</html>

