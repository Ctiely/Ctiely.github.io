<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" (一)强化学习 &middot;  Ctiely的个人博客" />
  	<meta property="og:site_name" content="Ctiely的个人博客" />
  	<meta property="og:url" content="https://ctiely.github.io/post/rl_introduction/" />


    <meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2019-04-24T19:58:43&#43;08:00" />
    <style type="text/css">
        p {text-indent: 2em;text-align: justify;margin: 0 auto;}
        a {text-decoration:none;}
        img {vertical-align:middle;}
    </style>



  <title>
      Reinforcement Learning &middot; 强化学习简介
  </title>

    <meta name="description" content="这是ctiely的个人博客" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://ctiely.github.io/images/user.png">
	  <link rel="apple-touch-icon" href="https://ctiely.github.io/images/apple-touch-icon.png" />
    
    <link rel="stylesheet" type="text/css" href="https://ctiely.github.io/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="https://ctiely.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Ctiely的个人博客" />
      
      
    
    <meta name="generator" content="Hugo 0.47" />

    <link rel="canonical" href="https://ctiely.github.io/post/rl_introduction/" />

    
<div id="particles-js"></div>
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="https://ctiely.github.io/js/particles.js"></script>
    <script src="../../js/click.js"></script>
</head>
<body class="nav-closed">

  


 <div class="site-wrapper">



<header class="main-header " style="background-image: url(https://ctiely.github.io/images/user.jpg)">

    <nav class="main-nav overlay clearfix">
        <a class="blog-logo" href="https://ctiely.github.io/"><img src="https://ctiely.github.io/images/user.png" alt="Blog Logo" /></a>
    </nav>
<div class="vertical">
        <div class="main-header-content inner">
            <h1 class="page-title">
              <a class="btn-bootstrap-2 title-scroll" href="#content">强化学习系列</a>
          </h1>
        </div>
</div>
    <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
</header>

  <main id="content" class="content" role="main">


  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">(一)强化学习简介</h1>
        <section class="post-meta">
        
	<time class="post-date" datetime="2019-04-24">
            2019-04-24
          </time>
        
         
        </section>
    </header>

    <p class="post-content">
        <p>机器学习可分为有监督学习，无监督学习以及强化学习。强化学习是机器学习的一个重要领域，强化学习可以简单理解为代理<b>(agent)</b>在
      环境<b>(environment)</b>中采取行动<b>(action)</b>并得到奖励<b>(reward)</b>来扩充自己的知识储备，使得自己在环境中有更好的表现，
      得到更多的回报<b>(return)</b>。
      <br>&nbsp;&nbsp;&nbsp;&nbsp;
      最近几年，强化学习大放异彩，被认为是最接近于人工智能的算法，著名的事件有<a href="https://deepmind.com/research/alphago/" target="_blank" style="text-decoration: underline"><b>AlphaGo</b></a>夺得
      围棋冠军、<a href="https://openai.com/" target="_blank" style="text-decoration: underline"><b>OpenAI</b></a>在dota2中击败半职业战队等。
  </p>
        <h5>1.强化学习过程</h5>
        <p>强化学习背后的思想就是agent通过与环境进行交互并从中学习，然后利用学习到的知识尽可能获得更多的回报。具体过程为：agent根据当前所
            处的状态<b>s<sup>t</sup></b>作出相应的决策/行动<b>a<sup>t</sup></b>后，环境进入到新的状态<b>s<sup>t+1</sup></b>，agent从
            环境中获得信息(<b>s<sup>t+1</sup></b>, <b>a<sup>t</sup></b>, <b>d<sup>t</sup></b>)，分别表示(下一个状态，当前行动的奖励，
            下一状态是否为终止状态)，agent再根据新的状态<b>s<sup>t+1</sup></b>进行决策/行动，交互过程反复进行，直到到达终止状态<b>s<sup>T</sup></b>，
            如图1所示；强化学习算法就是利用agent与环境交互得到的信息(<b>s<sup>t</sup></b>, <b>a<sup>t</sup></b>, <b>r<sup>t</sup></b>, <b>d<sup>t</sup></b>, <b>s<sup>t+1</sup></b>)建立模型，
            使agent在环境中获得最大的期望累积回报。<br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <img class="equations" src="../images/强化学习过程.png" alt="累积回报" width="310" height="285" style="text-align: center"><br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图1 代理与环境的交互过程<br>
        </p>
      <h5>2.强化学习算法中的关键概念</h5>
      <h6>(1)状态(state)和观测(observation)</h6>
      <p>
          状态是对当前环境的完全描述，而观测是对当前环境的部分描述；有些问题中，我们所能得到的观测就是状态，但多数情况下我们并不能获得状态，
          而只能观测到环境的部分信息，以英雄联盟(LOL)游戏来举例，状态就是当前时刻敌方和我方所处的情况，在游戏进行中，我们只能在地图中观测
          到我方视野可见区域的情况，对于我方不可见的区域(或称为被战争迷雾笼罩的地方)我们无法观测到那里的状况，因此没有办法获得当前环境的完
          全描述，只能获得当前环境的部分描述。在此之后，我们不再区分我们获得的信息是状态s还是观测o，一律称为状态。
      </p>
      <h6>(2)行动空间(action space)</h6>
      <p>
          不同的环境允许不同的种类的行动，所有合法行动的集合构成了该环境的行动空间。行动空间包括离散行动空间和连续行动空间，例如棋类游戏
          (围棋、五子棋)和街机类游戏(超级玛丽，打砖块)的行动空间是离散行动空间；自动驾驶任务中方向盘的旋转角度构成的行动空间是连续行动空间。
      </p>
      <h6>(3)策略(policy)</h6>
      <p>
          策略是状态s的函数，表示在当前状态下agent将要采取的行动，策略分为确定型策略和随机型策略。<br>
          确定型策略：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?a_t = \mu(s_t) \ \ \ or \ \ \ a_t = \mu_\theta(s_t)" width="270"/><br>
          随机型策略：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?a_t \sim \pi(\cdot|s_t) \ \ \ or \ \ \ a_t \sim \pi_\theta(\cdot|s_t)" width="300"/><br>
          随机型策略中，如果行动空间是离散型行动空间，则策略函数是一个离散型分布；否则是连续型分布，通常使用正态分布。
      </p>
      <h6 id="traj">(4)轨迹(trajectory)</h6>
      <p>
          轨迹是{状态，行动}对的序列，表示了agent与环境交互中所经历的过程。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?\tau = (s_0, a_0, s_1, a_1, ...,s_T)" width="300"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中<img src="http://latex.codecogs.com/svg.latex?s_0 \sim \rho_0 (\cdot)" width="100"/>表示环境随机进入到
          某个初始状态，这个状态是从状态分布中随机抽取得到的；当agent采取了某个行动后，环境会从当前状态转移到下一个状态，状态转移分为两种：确定性转移和随机性转移。<br>
          确定性转移：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?s_{t + 1} = f(s_t, a_t)" width="180"/><br>
          随机性转移：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?s_{t + 1} \sim P(\cdot| s_t, a_t)" width="210"/><br>
      </p>
      <h6>(5)奖励(reward)和回报(return)</h6>
      <p>
          奖励函数在强化学习算法中至关重要，因为它是决定模型朝什么方向发展的航标。奖励函数定义为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?r_t = R(s_t, a_t, s_{t+1}) \ \ \ or \ \ \ r_t = R(s_t, a_t) \ \ \ or \ \ \ r_t = R(s_t)" width="650"/><br>
          强化学习的任务就是要最大化整个轨迹上的期望累计回报，累计回报定义为：<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?R(\tau) = \sum_{t=0}^{T} r_t" width="121"/><br>
          然而在实际中，更常使用累计折扣回报。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t" width="150"/><br>
          累计折扣回报源于生活中的经验：未来的现金不如现在的现金值钱；以及数学处理上的方便：无穷级数直接求和可能不收敛，加上折扣因子后级数会收敛到某个数值。
      </p>
      <h6>(6)价值函数(value)和行动价值函数(action-value)</h6>
      <p>
          价值函数定义了agent从环境中的某个状态s出发，可以获得的期望回报，即<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?V^\pi(s) = E_{\tau \sim \pi} [R(\tau) | s_0 = s]" width="300"/><br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;行动价值函数定义了agent在环境中的某个状态s下采取行动a后可以获得的期望回报<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?Q^\pi(s, a) = E_{\tau \sim \pi} [R(\tau) | s_0 = s, a_0 = a]" width="400"/><br>
      </p>
      <h6>(7)优势函数(advantage)</h6>
      <p>
          优势函数定义了agent在环境中的某个状态s下采取行动a时，相对于随机采取行动所能获得的额外回报，即<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="http://latex.codecogs.com/svg.latex?A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)" width="300"/><br>
      </p>
      <h5>3.强化学习算法分类</h5>
      <p>
          强化学习算法可以分为两大类：基于模型的算法(Model-Based)和免模型的算法(Model-Free)，这里的模型指的是对环境进行建模，最著名的代
          表是DeepMind的AlphaZero。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们主要介绍免模型的强化学习算法。免模型的强化学习算法分为两大阵营，分别是
      <ul>
          <li>以DeepMind为代表的基于价值的强化学习算法</li>
          <li>以OpenAI为代表的基于策略的强化学习算法</li>
      </ul>
      强化学习算法生态体系如图2所示。<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <img src="../images/rl_algo.png" width="750"/><br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      图2 强化学习算法生态体系<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(来源:https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)
      </p>


  <footer class="post-footer">


    
    <figure class="author-image">

        <a class="img" href="https://ctiely.github.io/" style="background-image: url(https://ctiely.github.io/images/user.png)"><span class="hidden">ctiely</span></a>
    </figure>
    

    <section class="author">

        <p>@Author:<a href="https://github.com/Ctiely" style="text-decoration: underline" target="_blank"><b>ctiely</b></a></p>
        <p>@school:<b>Renmin University of China </b></p>
        <p>@Reference:<a href="https://spinningup.openai.com/en/latest/" style="text-decoration: underline" target="_blank"><b>Spinning Up</b></a></p>

    </section>
  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Ctiely的个人博客</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="https://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/syui/hugo-theme-air">hugo-theme-air</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/index.js"></script>

</body>
</html>

