<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" (一)强化学习 &middot;  Ctiely的个人博客" />
  	<meta property="og:site_name" content="Ctiely的个人博客" />
  	<meta property="og:url" content="https://ctiely.github.io/post/rl_introduction/" />


    <meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2018-08-25T19:58:43&#43;08:00" />
    <style type="text/css">
        p {text-indent: 2em;text-align: justify;margin: 0 auto;}
        a {text-decoration:none;}
        img {vertical-align:middle;}
    </style>
    
    

  <title>
      Reinforcement Learning &middot;  强化学习简介
  </title>

    <meta name="description" content="这是ctiely的个人博客" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://ctiely.github.io/images/user.png">
	  <link rel="apple-touch-icon" href="https://ctiely.github.io/images/apple-touch-icon.png" />
    
    <link rel="stylesheet" type="text/css" href="https://ctiely.github.io/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="https://ctiely.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Ctiely的个人博客" />
      
      
    
    <meta name="generator" content="Hugo 0.47" />

    <link rel="canonical" href="https://ctiely.github.io/post/rl_introduction/" />

    
<div id="particles-js"></div>
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="https://ctiely.github.io/js/particles.js"></script>   
</head>
<body class="nav-closed">

  


 <div class="site-wrapper">



<header class="main-header " style="background-image: url(https://ctiely.github.io/images/user.jpg)">

    <nav class="main-nav overlay clearfix">
        <a class="blog-logo" href="https://ctiely.github.io/"><img src="https://ctiely.github.io/images/user.png" alt="Blog Logo" /></a>
    </nav>
<div class="vertical">
        <div class="main-header-content inner">
            <h1 class="page-title">
              <a class="btn-bootstrap-2 title-scroll" href="#content">强化学习系列</a>
          </h1>
          <h2 class="page-description">强化学习(Reinforcement Learning)基础知识</h2>
        </div>
</div>
    <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
</header>

  <main id="content" class="content" role="main">


  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">(一)强化学习基础知识介绍</h1>
        <section class="post-meta">
        
	<time class="post-date" datetime="2018-08-25">
            2018-08-25
          </time>
        
         
        </section>
    </header>

    <p class="post-content">
        <p>机器学习可分为有监督学习，无监督学习以及强化学习。强化学习是机器学习的一大重要领域，强化学习可以简单理解为代理<b>(agent)</b>通过在
      环境<b>(environment)</b>中采取行动<b>(action)</b>并得到结果<b>(reward)</b>来扩充自己的知识，以便自己能够在环境中有更好的表现。
      <br>&nbsp;&nbsp;&nbsp;&nbsp;
      最近几年，强化学习大放异彩，被认为是最接近于人工智能的算法，著名的事件有<a href="https://deepmind.com/research/alphago/" target="_blank" style="text-decoration: underline"><b>AlphaGo</b></a>夺得
      围棋冠军、<a href="https://openai.com/" target="_blank" style="text-decoration: underline"><b>OpenAI</b></a>在dota2中击败半职业战队等。</p>
        <h5>(1)强化学习过程</h5>
        <p>强化学习背后的思想就是代理通过与环境进行交互并从中学习，然后利用学习到的知识尽可能获得更多的奖励。通过模仿人类的学习过程使得代
            理能够在与环境进行交互的同时进行学习，假设一个刚出生的婴儿，当他看到一团火时，他选择靠近这团火，这时他感到温暖，他认为这是一个
            正的奖励；当他再继续靠近时，他被火烧伤，获得了一个负的反馈。从上面与环境的交互过程中，人类从环境中学习到火可以带来温暖，但是离
            的太近会使人受伤。强化学习过程也类似于上述人的学习过程，它是从代理的行为中进行学习的一种计算方法。</p>
        <p>假设代理正在玩一款叫做超级玛丽的游戏，那么强化学习的过程可以被刻画为进行一系列的循环:
        <ol>
        <li>代理从环境中获得了某个状态<b>S<sub>0</sub></b>(在我们的例子中就是超级玛丽游戏开始后的某一个画面(某一帧))</li>
        <li>在状态<b>S<sub>0</sub></b>，代理采取某个行动<b>A<sub>0</sub></b>(比如向右移动)</li>
        <li>代理就会转移到下一个状态<b>S<sub>1</sub></b>(游戏的下一个画面(下一帧))</li>
        <li>环境反馈给代理一个回报(如果没有死亡，获得回报+1)</li>
        </ol>
        强化学习循环进行并输出一系列的[状态，行动，回报]，代理的目标就是最大化期望累积回报。
        在第<b>t</b>步的累积回报可以写为:<br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq1.png" alt="累积回报" width="160" height="25" style="text-align: center"><br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        然而在实际中，第<b>t</b>步的累积回报并不是每一步的回报的简单求和。对于更长期的回报，能够获得它的可能性要小于获得短期回报的可能性，因此需要
        在长期回报中添加一个折扣因子<b>&gamma;</b>，取值范围在<b>0～1</b>之间，此<b>&gamma;</b>越大就表明折扣力度越小，意味着代理更倾向于关注长期回报；反之，则更倾向于关注短期回报。
            <br>因此，期望累积折扣回报写为:<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq2.png" alt="期望累积折扣回报" width="250" height="30"><br>
        </p>
        <h5>(2)强化学习的两类任务</h5>
        <p>任务是强化学习解决的问题，任务可以分为两种类型:
            <ul>
            <li>回合制任务</li>
            <li>连续型任务</li>
            </ul>
        一般的游戏都是回合制任务，比如说超级玛丽、dota2等游戏都有一个开始，并且游戏最终会结束，而一个回合就相当于一场游戏从开始到结束。
      比较典型的连续型任务就是股票自动交易，股票自动交易可以看作是一个连续不断的过程，代理无时无刻在进行着股票交易(包括股票购买与出售)，
      没有开始点也没有结束点，代理始终保持运行直到我们人为将它关闭。
        </p>
        <h5>(3)价值函数更新算法</h5>
      <p>价值函数的概念我将在第(<a href="#part4"><b>4</b></a>)部分进行介绍，现在我们先来看一下强化学习是采用什么方法来对价值函数(可以
          类比理解为有/无监督学习中的模型参数)进行更新的。
            <br>代理有两种方法可以从环境中进行学习:
            <ul>
              <li>蒙特卡洛方法<b>(Monte Carlo)</b></li>
              <li>时序差分方法<b>(Temporal Difference Learning)</b></li>
            </ul>
      <h6 style="font-size: 19px">1.蒙特卡洛方法</h6>
      <p>每一回合的结束时刻进行计算，求得最大期望未来收益。代理在每一回合的结束时刻去计算此回合的累积回报，然后根据新得到的知识进行更新，代理将会在每一回合的迭代后作出更好的决策。
          <br>蒙特卡洛方法下，价值函数<b>V</b>的更新公式写为:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq3.png" alt="蒙特卡洛方法下，价值函数V的更新公式" width="240" height="27"><br>
          式中左侧的<b>V(S<sub>t</sub>)</b>表示在状态<b>S<sub>t</sub></b>时能够获得的最大期望未来回报的更新值，右侧的<b>V(S<sub>t</sub>)</b>表示在
          状态<b>S<sub>t</sub></b>时能够获得的最大期望未来回报的当前值，<b>&alpha;</b>是学习率，<b>G<sub>t</sub></b>是前面讲到的累积期望折扣回报。
      </p>
      <h6 style="font-size: 19px" id="time_difference">2.时序差分方法</h6>
      <p>与蒙特卡洛方法正好相反，时序差分法在每一步(代理每进行一个行动)都对回报进行更新，而不是等待整个回合结束以后再去更新。这种方法叫做<b>TD(0)</b>或
          者一步时序差分法，此种方法使代理能够在每一步完成后立刻对价值函数进行更新。<br>
          时序差分方法下，价值函数<b>V</b>的更新公式写为:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq4.png" alt="时序差分方法下，价值函数V的更新公式" width="360" height="27"><br>
          式中左侧的<b>V(S<sub>t</sub>)</b>仍然表示在状态<b>S<sub>t</sub></b>时能够获得的最大期望未来回报的更新值，右侧的<b>V(S<sub>t</sub>)</b>表示在状态
          <b>S<sub>t</sub></b>时能够获得的最大期望未来回报的当前值，<b>&alpha;</b>是学习率，式中的<b>R<sub>t+1</sub></b>表示在状态<b>S<sub>t</sub></b>下作出某个
          行动后可以获得的即期回报，<b>V(S<sub>t+1</sub>)</b>表示作出行动并到达新状态以后，从新状态出发可以获得的最大期望未来回报，<b>&gamma;</b>是折
          扣率，<b>R<sub>t+1</sub>+&gamma;V(S<sub>t+1</sub>)</b>被称为<b>TD</b>目标。该式可以理解为当前状态下的最大期望未来回报是利用新回合中可以获得的
          最大期望未来回报对原始的最大期望未来回报进行更新。
      </p>
      <p>从上面两种方法的价值函数更新公式中可以看到，在每个回合(或者每一次行动)结束后，我们都会通过在当前回合中(或者当前这次行动下)经历的[状态，行动，回报]
          来对所有状态下的最大期望未来回报进行更新，之后的部分将会介绍代理将如何根据多轮回合(或者多步行动)后迭代计算出来的价值函数作出决策。</p>
            </p>
      <h5 id="part4">(4)强化学习解决问题的常用方法</h5>
      <p>代理通常利用两种方法来解决强化学习所需要解决的问题，分别是基于价值的方法和基于策略的方法。
      <h6 style="font-size: 19px" id="value_based">1.基于价值的方法</h6>
      <p>基于价值的强化学习的目标就是最优化价值函数，价值函数就是代理在每个状态下所能够得到的最大期望未来回报，表示代理从状态<b>S<sub>t</sub></b>开始，
          预期在未来可以获得的累积回报。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq5.png" alt="累积期望回报" width="380" height="35"><br>
          代理将会根据计算得到的价值函数来作出决策，通常的决策是在每一步选择价值函数最大的状态。
      </p>
      <h6 style="font-size: 19px">2.基于策略的方法</h6>
      <p>基于策略的强化学习的目标是直接优化一个策略函数<b>&pi;(s)</b>而不是利用价值函数作出决策。策略定义了代理在某个给定的状态下将会采取
          哪种行动，通过学习策略函数，代理可以将状态映射到每个状态所对应的最优行动。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq6.png" alt="策略函数" width="90" height="30"><br>
          通常有两种类型的策略:确定型和随机型。
          <ul>
              <li>确定型策略:对于每个给定的状态都只有唯一确定的行动。</li>
              <li>随机型策略:对于每个给定的状态，策略函数会产生该状态所对应的行动的概率分布。</li>
          </ul>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="equations" src="https://ctiely.github.io/images/equations/rl/introduction_eq7.png" alt="随机策略函数" width="250" height="30"><br>
        迭代得到了策略函数后，代理就可以在给定的状态下根据决策函数作出决策。
      </p>
      </p>
    </section>


  <footer class="post-footer">


    
    <figure class="author-image">

        <a class="img" href="https://ctiely.github.io/" style="background-image: url(https://ctiely.github.io/images/user.png)"><span class="hidden">ctiely</span></a>
    </figure>
    

    <section class="author">

        <p>@Author:<a href="https://github.com/Ctiely" style="text-decoration: underline" target="_blank"><b>ctiely</b></a></p>
        <p>@school:<b>Renmin University of China </b>
        </p>

</section>


    <!--
    <section class="share">
      <h4>Share this post</h4>
      <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=Test&amp;url=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
          onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <span class="hidden">Twitter</span>
      </a>
      <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
          onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <span class="hidden">Facebook</span>
      </a>
      <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
         onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
          <span class="hidden">Google+</span>
      </a>
    </section>
    -->

    
    
    

  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Ctiely的个人博客</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="https://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/syui/hugo-theme-air">hugo-theme-air</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/index.js"></script>

</body>
</html>

