<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" (二)强化学习 &middot;  Ctiely的个人博客" />
  	<meta property="og:site_name" content="Ctiely的个人博客" />
  	<meta property="og:url" content="https://ctiely.github.io/post/rl_q_learning/" />


    <meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2018-08-26T19:58:43&#43;08:00" />
    <style type="text/css">
        p {text-indent: 2em;text-align: justify;margin: 0 auto;}
        a {text-decoration:none;}
        img {vertical-align:middle; text-align: center;}
    </style>
    
    

  <title>
      Reinforcement Learning &middot;  Q-learning
  </title>

    <meta name="description" content="这是ctiely的个人博客" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://ctiely.github.io/images/favicon.ico">
	  <link rel="apple-touch-icon" href="https://ctiely.github.io/images/apple-touch-icon.png" />
    
    <link rel="stylesheet" type="text/css" href="https://ctiely.github.io/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="https://ctiely.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Ctiely的个人博客" />
      
      
    
    <meta name="generator" content="Hugo 0.47" />

    <link rel="canonical" href="https://ctiely.github.io/post/rl_q_learning/" />

    
<div id="particles-js"></div>
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="https://ctiely.github.io/js/particles.js"></script>   
</head>
<body class="nav-closed">

  


 <div class="site-wrapper">



<header class="main-header " style="background-image: url(https://ctiely.github.io/images/user.jpg)">

    <nav class="main-nav overlay clearfix">
        <a class="blog-logo" href="https://ctiely.github.io/"><img src="https://ctiely.github.io/images/user.png" alt="Blog Logo" /></a>
    </nav>
<div class="vertical">
        <div class="main-header-content inner">
            <h1 class="page-title">
              <a class="btn-bootstrap-2 title-scroll" href="#content">强化学习系列</a>
          </h1>
          <h2 class="page-description">强化学习(Reinforcement Learning)之Q-learning</h2>
        </div>
</div>
    <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
</header>

  <main id="content" class="content" role="main">


  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">(二)强化学习之Q-learning</h1>
        <section class="post-meta">
        
	<time class="post-date" datetime="2018-08-26">
            2018-08-26
          </time>
        
         
        </section>
    </header>

    <p class="post-content">
      <p><b>Q-learning</b>是经典的强化学习算法，它是
      一种<a href="../../post/rl_introduction/#value_based" target="_blank" style="text-decoration: underline"><b>基于价值的算法</b></a>。</p>
      <p>假设你现在是一名骑士，四个敌人固定不动，每一步你都可以选择向四个方向前进一格，你的职责是避开图中的敌人并且尽快赶到城堡。这样的一个
          问题可以由一个得分系统来描述。
      </p>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <img src="images/ex1.png" width="300" height="300" style="">
      <ul>
          <li>每一步你都会得到-1分(丢失分数就意味着你要尽可能快的到达城堡)。</li>
          <li>碰到敌人游戏失败，你将被扣除100分，该回合结束。</li>
          <li>到达城堡后游戏获得胜利，你将会获得100分，该回合结束。</li>
      </ul>
      <h5>(1)Q值表(Q-table)</h5>
      <p>我们可以通过构造一个<b>Q</b>值表(<b>Q-table</b>，<b>Q</b>代表着行动的质量(<b>quality</b>))，<b>Q</b>值表中的每个元素衡量
          了在给定状态采取给定行动时将会获得的最大期望未来回报，因此我们可以让代理根据<b>Q</b>值表去选择每个状态下的最优行动，并最终获取游戏胜利。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <b>Q</b>值表的行表示环境中所有的状态，列表示在给定状态下可以采取的行动，在上面的案例中，每一个格子就代表一个状态，
          因此<b>Q</b>值表中共有25行；在每个状态下，代理最多有四种行动(上、下、左、右)，因此<b>Q</b>值表共有4列(当然，在某些状态下，
          有些行动是被禁止的，这将被体现在<b>Q</b>值表的元素中)。请注意，此算法的目的是通过<b>Q</b>值表进行决策，这是基于价值的算法。我们也
          可以把<b>Q</b>值表理解为游戏的作弊器，在游戏中的某个状态下(<b>Q</b>值表中对应的那一行)，代理能够从作弊器中知道下一步往哪
          里走(那一行所对应的最大值)可以获得最大回报，因此只要我们获得了<b>Q</b>值表，我们就可以解决上述的骑士问题。
      </p>
      <h5>(2)利用Q-learning算法更新Q值表</h5>
      <p>行动价值函数(<b>Q</b>函数)接收两个输入，分别是状态和行动，它会返回在该状态下采取该行动时将会获得的期望回报，我们可以把<b>Q</b>函数假想
          为一个作弊器，我们指定某一个状态和该状态下的一个行动，就可以通过这个作弊器返回一个得分(<b>Q</b>值)，那么我们就可以根据得分大小来选择这个
          状态下的行动。<br>
          <b>Q</b>值写为:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <img src="images/qvalue.png" width="420" height="30" alt="Q值公式"></p>
      初始<b>Q</b>值表的每个元素都被赋值为<b>0</b>，我们可以通过代理与环境的交互来获得新的信息，并且根据这些交互信息利用<b>Bellman</b>方程迭代
      更新<b>Q</b>值表中的每个元素<b>Q(s,a)</b>。
      <img src="images/q_function.png" width="680" height="150"><br>
      <h6 style="font-size: 19px">1.Q值表更新流程图</h6>
      <img src="images/update1.png" width="680" height="75">
      <h6 style="font-size: 19px" id="q_learning">2.Q-learning更新算法</h6>
      <ol>
          <li><b>初始化Q值表</b><br></li>
          初始化所有的<b>Q(s,a)</b>为<b>0</b>，<b>Q</b>值表共有<b>m</b>行(<b>m</b>个状态)，<b>n</b>列(每个状态对应<b>n</b>个行动)。
          <li><b>选择行动a</b><br>
              在当前状态<b>s</b>下基于当前的<b>Q</b>值表选择某个行动，此处我们将根据<b>epsilon greedy strategy</b>来选取行动。首先，我们需要定义一个
              探索率<b>"epsilon"</b>，这个探索率将决定我们将以多大的概率随机选取一个行动。比如探索率取0.5，就意味着我们将有50%的概率随机
              选取一个行动，有50%的概率根据当前的<b>Q</b>值表去选取一个<b>Q</b>值最大的行动。通常我们在游戏开始时选取探索率为100%，这样我们的代
              理就会在所有<b>Q</b>值都为<b>0</b>的情况下对环境开始进行探索。随着回合数的增加，我们需要降低探索率的取值，让代理有更大的概
              率去依赖<b>Q</b>值表选取行动。</li>
          <li><b>执行行动并获得回报</b><br>
              在状态<b>s</b>下采取行动<b>a</b>，并从环境中获得下一个状态<b>s'</b>和回报<b>r</b>，接下来我们将会利用<b>s'</b>和<b>r</b>去更新<b>Q(s,a)</b>。</li>
          <li><b>更新Q值表中对应位置的Q值</b><br>
              利用<b>Bellman</b>方程更新<b>Q(s,a)</b>:<br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <img src="images/eq1.png" width="600" height="120"><br>
              式中左侧表示新的<b>Q</b>值；<b>&alpha;</b>表示学习率，学习率决定了我们会多快地更新原有的<b>Q</b>值，很显然，当学习率为<b>1</b>时，我们
              最快速地将原有的<b>Q</b>值更新为新计算得到的<b>Q</b>值；<b>R(s,a)</b>表示在状态<b>s</b>处采取行动<b>a</b>所获得的回报；<b>&gamma;</b>表示折
              扣因子；<b>maxQ'(s',a')</b>表示下一个状态下所能获得的最大期望未来回报，这个公式其实就是上一节介
              绍的<a href="../../post/rl_introduction/#time_difference" style="text-decoration: underline" target="_blank"><b>时序差分法</b></a>的价值函数更新公式。
          </li>
          <li><b>达到最大回合数后停止迭代</b><br>
              当代理死亡或者获得胜利时，算作一个回合结束，当代理进行了许多轮回合并且达到了我们给定的最大回合次数阈值时，我们认为代理已经获得了一个迭
              代好的优秀的<b>Q</b>值表，接下来代理就可以利用这个计算好的<b>Q</b>值表来作出决策以获得胜利。
          </li>
      </ol>
      <h5>(3)利用numpy实现Q-learning算法</h5>
      <p>首先导入<b>numpy</b>、<b>gym</b>以及<b>random</b>模块，<b>numpy</b>模块将会用来计算并更新<b>Q</b>值表，<b>gym</b>模块用来生成我们需要的游戏环境，这样
          我们就不需要自己写一个游戏环境供我们训练模型，<b>random</b>模块将用来生成随机数，并与探索率<b>"epsilon"</b>进行比较，用来控制我们是否随机选取行动。</p>
      <pre>import numpy as np<br>import gym<br>import random</pre>
      <p>接下来我们设置一些学习参数，包括以下这些参数。</p>
      <pre>env = gym.make("Taxi-v2") #利用gym生成我们需要的游戏环境

action_size = env.action_space.n
state_size = env.observation_space.n

Qtable = np.zeros((state_size, action_size)) #初始化Q值表(状态数, 行动数)

total_episodes = 15000        # 总回合数
learning_rate = 0.8           # 学习率&alpha;
max_steps = 99                # 每一回合最大游戏进行次数
gamma = 0.95                  # 折扣因子

# 探索率设置
epsilon = 1.0                 # 探索率
max_epsilon = 1.0             # 最大探索率
min_epsilon = 0.01            # 最小探索率
decay_rate = 0.005            # 每一回合结束后探索率通过指数衰减
</pre>
      <p>然后，我们利用<b>Q-learning</b>对初始化的<b>Q</b>值表进行迭代更新，每一步更新都遵循前一部分
          介绍的<a href="#q_learning" style="text-decoration: underline"><b>Q-learning更新算法</b></a>。</p>
      <pre>rewards = []
for episode in range(total_episodes):
    state = env.reset()     #类似于每一回合结束后，进行下一次游戏时需要点击的开始游戏按钮
    done = False
    total_rewards = 0

    for step in range(max_steps):
        exp_exp_tradeoff = random.uniform(0, 1)

        if exp_exp_tradeoff > epsilon:
            action = np.argmax(Qtable[state, :])
        else:
            action = env.action_space.sample()

        new_state, reward, done, info = env.step(action)     #让游戏中的任务执行action动作，并返回相应的信息(新的状态s', 回报r, 游戏是否结束, 游戏信息)

        #更新Q(s,a)
        Qtable[state, action] = Qtable[state, action] + learning_rate * (reward +
                                gamma * np.max(Qtable[new_state, :]) - Qtable[state, action])

        total_rewards += reward

        state = new_state       #让我们的角色进入到下一状态

        if done:
            break
    #每一回合结束后衰减探索率
    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)
    rewards.append(total_rewards)

print ("Score over time: " +  str(sum(rewards)/total_episodes))
print(Qtable)</pre>

      <p>最后，我们就可以使用已经迭代好的<b>Q</b>值表去进行游戏，然后查看游戏的结果。</p>
      <pre>env.reset()

for episode in range(1):
    state = env.reset()
    done = False
    print("*" * 15)
    print("EPISODE ", episode)
    env.render()    #用来展示游戏状态，相当于游戏画面的每一帧
    for step in range(max_steps):
        action = np.argmax(Qtable[state, :])

        new_state, reward, done, info = env.step(action)
        env.render()
        if done:
            #env.render()
            print("Number of steps", step + 1)
            break
        state = new_state

env.close()</pre>
      <a href="https://github.com/Ctiely/reinforcement_learning/blob/master/q_learning_taxi.py" target="_blank" style="text-decoration: underline; color: #e4c05c;"><b>获取源代码</b></a><br>
      游戏结果展示:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <img src="images/q_learning.gif" width="100" height="120">



      </p>
    </section>


  <footer class="post-footer">


    
    <figure class="author-image">

        <a class="img" href="https://ctiely.github.io/" style="background-image: url(https://ctiely.github.io/images/user.png)"><span class="hidden">ctiely</span></a>
    </figure>
    

    <section class="author">

        <p>@Author:<a href="https://github.com/Ctiely" style="text-decoration: underline" target="_blank"><b>ctiely</b></a></p>
        <p>@school:<b>Renmin University of China </b>
        </p>

</section>


    <!--
    <section class="share">
      <h4>Share this post</h4>
      <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=Test&amp;url=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
          onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <span class="hidden">Twitter</span>
      </a>
      <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
          onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <span class="hidden">Facebook</span>
      </a>
      <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
         onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
          <span class="hidden">Google+</span>
      </a>
    </section>
    -->

    
    
    

  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Ctiely的个人博客</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="https://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/syui/hugo-theme-air">hugo-theme-air</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/index.js"></script>

</body>
</html>

