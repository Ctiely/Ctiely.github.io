<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" (三)强化学习 &middot;  Ctiely的个人博客" />
  	<meta property="og:site_name" content="Ctiely的个人博客" />
  	<meta property="og:url" content="https://ctiely.github.io/post/rl_deep_q_learning/" />


    <meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2018-08-31T19:58:43&#43;08:00" />
    <style type="text/css">
        p {text-indent: 2em;text-align: justify;margin: 0 auto;}
        a {text-decoration:none;}
        img {vertical-align:middle; text-align: center;}
    </style>
    
    

  <title>
      Reinforcement Learning &middot;  Deep q-learning
  </title>

    <meta name="description" content="这是ctiely的个人博客" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://ctiely.github.io/images/user.png">
	  <link rel="apple-touch-icon" href="https://ctiely.github.io/images/apple-touch-icon.png" />
    
    <link rel="stylesheet" type="text/css" href="https://ctiely.github.io/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="https://ctiely.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Ctiely的个人博客" />
      
      
    
    <meta name="generator" content="Hugo 0.47" />

    <link rel="canonical" href="https://ctiely.github.io/post/rl_q_learning/" />

    
<div id="particles-js"></div>
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="https://ctiely.github.io/js/particles.js"></script>
    <script src="../../js/click.js"></script>
</head>
<body class="nav-closed">

  


 <div class="site-wrapper">



<header class="main-header " style="background-image: url(https://ctiely.github.io/images/user.jpg)">

    <nav class="main-nav overlay clearfix">
        <a class="blog-logo" href="https://ctiely.github.io/"><img src="https://ctiely.github.io/images/user.png" alt="Blog Logo" /></a>
    </nav>
<div class="vertical">
        <div class="main-header-content inner">
            <h1 class="page-title">
              <a class="btn-bootstrap-2 title-scroll" href="#content">强化学习系列</a>
          </h1>
          <h2 class="page-description">强化学习(Reinforcement Learning)<br>—Deep q-learning</h2>
        </div>
</div>
    <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
</header>

  <main id="content" class="content" role="main">


  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">(三)强化学习之Deep q-learning</h1>
        <section class="post-meta">
        
	<time class="post-date" datetime="2018-08-31">
            2018-08-31
          </time>
        
         
        </section>
    </header>

    <p class="post-content">
      <p>上一部分，我们学习了<a href="../rl_q_learning/#q-learning" target="_blank" style="text-decoration: underline"><b>Q-learning</b></a>，它通过
      迭代更新得到<b>Q</b>值表，然后利用<b>Q</b>值表作出决策并采取行动，但是<b>Q-learning</b>有着十分明显的缺点:当状态空间十分庞大时，更新迭代<b>Q</b>值表是低效的并且
      几乎变得不可行。</p>
      <p>我们将使用深度神经网络作为估计<b>Q</b>值表的模型，这个神经网络接收状态作为参数，并能够返回该状态下对应的每个行动的<b>Q</b>值的预测值。
      </p>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <img src="images/deep_q_learning.png" width="500" height="400" style=""><br>
      使用了深度神经网络改进<b>Q-learning</b>算法，就得到了<b>Deep Q-learning</b>算法，它和<b>Q-learning</b>算法的不同在于它不是直接去计算<b>Q</b>值表，而是
      利用一个深层网络模型去拟合<b>Q</b>值表，这就类似于有监督学习中的回归模型，有了这个模型以后，我们就可以输入状态取值<b>s</b>，然后获得模型的输出值(该状态下所有行动对应的<b>Q</b>值)。
      <h5>(1)例解Deep Q-learning</h5>
      <p>我们可以通过构建一个卷积神经网络来预测各个状态所对应的<b>Q</b>值，模型的结构如下:<br>
        <img src="images/neural.png" width="720" height="150">
      <h6 style="font-size: 19px">1.数据预处理</h6>
      通过数据预处理，我们可以降低模型输入数据的复杂度，提高模型的训练速度。
      <ol>
          <li>由于颜色对于代理判断敌人所在位置基本没有作用，因此可以通过将彩色图片(<a href="https://en.wikipedia.org/wiki/RGB_color_model" target="_blank" style="text-decoration: underline"><b>RGB</b></a>)转换为灰
              度图片来降低输入数据的维度，使我们的输入通道数由3变成1；</li>
          <li>天花板对我们的任务也没有帮助，因此可以裁剪掉图片上方的天花板；</li>
          <li>通过堆叠连续的四幅图像可以使代理识别出游戏角色的移动方向，这样可以让我们的模型获得更多的信息。</li>
      </ol>
      <h6 style="font-size: 19px">2.卷积层</h6>
      对于图像数据，常用的深度学习模型是卷积神经网络，它可以很好的利用图像数据的性质，识别到图像区块所带来的信息，这里，我们的模型选用三层卷
      积层，并且利用<a href="https://en.wikipedia.org/wiki/Batch_normalization" target="_blank" style="text-decoration: underline"><b>Batch Normalization</b></a>对数据
      进行预处理，这样可以使得神经网络模型更快地收敛，激活函数选用<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" style="text-decoration: underline"><b>ELU</b></a>。
      <h6 style="font-size: 19px">3.全连接层</h6>
      首先使用<b>flatten</b>层将3&times;3&times;128的图片转换成向量，然后通过两层全连接层输出为3维向量，每个维度分别对应着3个行动(左、右、开枪)的<b>Q</b>值。
      </p>
      <h5>(2)模型的不足及改进</h5>
      模型的训练过程中会存在一些问题:
      <ul>
          <li>在学习的过程中，模型会忘记以前的经验</li>
          <li>序贯的输入数据之间存在很强的相关性</li>
      </ul>
      <p>这些问题会使得我们的模型不能具有很好的泛化能力，在不同的环境下(<b>level</b>)表现很差，或者在某些情况下无法学习到罕见情况的处理办法。
          因此，我们需要对模型的训练过程进行一些改进，以使我们的模型可以更好地应对不同的场景。<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          游戏角色每操作一步，就会获得一个元组(当前状态，行动，回报，新的状态)，我们通过这些元组来训练模型。
      <ul>
            <li>我们可以通过建立一个经验库来解决第一个问题，我们将每次操作后得到的元组加入到我们的经验库中，这样我们就可以在训练时随机的从经验库中
                选取一些元组来进行训练，这样就可以防止模型在训练过程中只记住当前训练数据的信息，而忘记了以前训练数据的信息。
            </li>
      </ul>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      另一个严重的问题是，每次模型训练的输入数据都是连续输入的，那么我们前一次的输入数据与当前的输入数据就会存在很大的相关性，使用这样的
      数据训练模型，会让模型学习到不应该学习到的数据相关性，这样就可能使我们的模型没有办法应对那些出现不频繁的事件。
      <ul>
              <li>我们可以预先对环境进行随机探索来建立经验库，并且在训练过程中时刻扩充经验库，每次训练样本都是从经验库中随机抽取出来的<b>batch</b>，根据<b>batch</b>对模型进行更新。
              </li>
      </ul>
      总结来说，我们首先通过预训练过程，从与环境随机交互的过程中获取一些训练数据元组并将它们存储在经验库中，在训练阶段，我们会利用<a href="../rl_q_learning/#epsilon-greedy-strategy" target="_blank" style="text-decoration: underline"><b>epsilon greedy strategy</b></a>来决定我们在当前步是
      随机选取行动还是根据神经网络计算得到的<b>Q</b>值选取行动，得到了新的数据元组后，也需要将这部分元组存储在经验库中，而我们用来训练的数据，是
      从数据库中随机选取出来的批量数据(<b>batch</b>)，不是从经验库中按照时间顺序选取的数据。
          </p>
      <h5>(3)模型的更新</h5>
      <p><b>Q-learning</b>是利用<b>Bellman</b>方程来更新<b>Q</b>值表中的<b>Q</b>值，<b>Deep Q-learning</b>是通过减小<b>Q_target</b>(从下一个状态所能获得的最大可能回报)与<b>Q_value</b>(当前状态下对<b>Q</b>值的预测)之间的差距来更新网络的参数。
          当<b>Q_target</b>与<b>Q_value</b>的差距能够一直保持比较小时，说明模型能够比较好的预测出各个行动所对应的<b>Q</b>值，所以<b>Q_target</b>与<b>Q_value</b>两者之间的<b>MSE</b>即是模型需要优化的目标。<br>
          如果采用简单的梯度下降算法，那么网络的参数变化量为:<br>
      <img src="images/update.png" width="700" height="200">
          通常情况下，我们会利用<b>Adam</b>下降算法去更新网络中的参数。<br>
          在整个网络更新中，我们有<b>两个</b>重要的步骤:
      <ol>
          <li>我们需要在预训练阶段以及训练阶段将得到的元组(当前状态，行动，回报，新的状态)扩充至我们的经验库中。
          </li>
          <li>我们在模型训练过程中，每一步参数更新所使用的训练数据都是从经验库中选取出来的批数据，批数据并不是按照时间顺序获得的，而是从经验库中随机抽取出来的，这样的批数据就可以很好的解决模型在训练中遇到的两大影响效果的问题。
          </li>
      </ol>
      </p>

    </section>


  <footer class="post-footer">


    
    <figure class="author-image">

        <a class="img" href="https://ctiely.github.io/" style="background-image: url(https://ctiely.github.io/images/user.png)"><span class="hidden">ctiely</span></a>
    </figure>
    

    <section class="author">

        <p>@Author:<a href="https://github.com/Ctiely" style="text-decoration: underline" target="_blank"><b>ctiely</b></a></p>
        <p>@school:<b>Renmin University of China </b>
        </p>

</section>


    <!--
    <section class="share">
      <h4>Share this post</h4>
      <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=Test&amp;url=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
          onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <span class="hidden">Twitter</span>
      </a>
      <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
          onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <span class="hidden">Facebook</span>
      </a>
      <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=https%3a%2f%2fctiely.github.io%2fpost%2ftest%2f"
         onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
          <span class="hidden">Google+</span>
      </a>
    </section>
    -->

    
    
    

  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Ctiely的个人博客</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="https://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/syui/hugo-theme-air">hugo-theme-air</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://ctiely.github.io/js/index.js"></script>

</body>
</html>

